{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer Spark Streaming → Bronze Layer\n",
    "\n",
    "Lit le flux Kafka en temps réel et écrit les données brutes dans Delta Lake (couche Bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 : Configuration Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# Configuration\n",
    "KAFKA_SERVERS = \"kafka:29092\"\n",
    "GARAGE_ENDPOINT = \"http://garage:3900\"\n",
    "GARAGE_ACCESS_KEY = \"GK...\"  # À remplacer\n",
    "GARAGE_SECRET_KEY = \"...\"    # À remplacer\n",
    "\n",
    "# Spark session avec Delta Lake, Kafka et S3 (Garage)\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Kafka to Bronze\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", GARAGE_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", GARAGE_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", GARAGE_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder,\n",
    "    extra_packages=[\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\", \"org.apache.hadoop:hadoop-aws:3.3.4\"]\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark session créée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Lecture du stream Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture stream Kafka - Stories\n",
    "stories_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVERS) \\\n",
    "    .option(\"subscribe\", \"hn-stories\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Lecture stream Kafka - Comments\n",
    "comments_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVERS) \\\n",
    "    .option(\"subscribe\", \"hn-comments\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Streams Kafka connectés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 : Parsing des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, BooleanType\n",
    "\n",
    "# Schéma Stories\n",
    "story_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"by\", StringType()),\n",
    "    StructField(\"time\", LongType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"score\", IntegerType()),\n",
    "    StructField(\"descendants\", IntegerType()),\n",
    "    StructField(\"kids\", ArrayType(IntegerType()))\n",
    "])\n",
    "\n",
    "# Schéma Comments\n",
    "comment_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"by\", StringType()),\n",
    "    StructField(\"time\", LongType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"parent\", IntegerType()),\n",
    "    StructField(\"kids\", ArrayType(IntegerType())),\n",
    "    StructField(\"deleted\", BooleanType()),\n",
    "    StructField(\"dead\", BooleanType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser les stories\n",
    "stories_parsed = stories_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), story_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"time\"))) \\\n",
    "    .withColumn(\"_ingested_at\", current_timestamp())\n",
    "\n",
    "# Parser les comments\n",
    "comments_parsed = comments_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), comment_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"time\"))) \\\n",
    "    .withColumn(\"_ingested_at\", current_timestamp())\n",
    "\n",
    "print(\"Données parsées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : Écriture vers Bronze (avec déduplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "BRONZE_PATH = \"s3a://bronze/hackernews\"\n",
    "CHECKPOINT_PATH = \"s3a://bronze/checkpoints\"\n",
    "\n",
    "def upsert_to_delta(batch_df, batch_id, table_path):\n",
    "    \"\"\"Upsert (MERGE) vers Delta Lake - déduplique sur id\"\"\"\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Si la table existe, faire un MERGE\n",
    "    if DeltaTable.isDeltaTable(spark, table_path):\n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            batch_df.alias(\"source\"),\n",
    "            \"target.id = source.id\"\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    else:\n",
    "        # Première écriture : créer la table\n",
    "        batch_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Stories → Bronze (avec MERGE)\n",
    "stories_query = stories_parsed.writeStream \\\n",
    "    .foreachBatch(lambda df, id: upsert_to_delta(df, id, f\"{BRONZE_PATH}/stories\")) \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/stories\") \\\n",
    "    .start()\n",
    "\n",
    "# Stream Comments → Bronze (avec MERGE)\n",
    "comments_query = comments_parsed.writeStream \\\n",
    "    .foreachBatch(lambda df, id: upsert_to_delta(df, id, f\"{BRONZE_PATH}/comments\")) \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/comments\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streams démarrés vers Bronze (avec déduplication)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Monitoring du stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Afficher les stats toutes les 10 secondes\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\n--- Stats ---\")\n",
    "        print(f\"Stories: {stories_query.status}\")\n",
    "        print(f\"Comments: {comments_query.status}\")\n",
    "        time.sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nArrêt des streams...\")\n",
    "    stories_query.stop()\n",
    "    comments_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les données Bronze\n",
    "spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/stories\").show(5, truncate=False)\n",
    "spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/comments\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
