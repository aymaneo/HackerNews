{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer Spark Streaming → Bronze Layer\n",
    "\n",
    "Lit le flux Kafka en temps réel et écrit les données brutes dans Delta Lake (couche Bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 : Configuration Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Configuration\n",
    "KAFKA_SERVERS = \"kafka:9092\"\n",
    "GARAGE_ENDPOINT = \"http://garage:3900\"\n",
    "GARAGE_ACCESS_KEY = \"GKa25124b4fd82613c063217f3\"\n",
    "GARAGE_SECRET_KEY = \"008126399688f9b1efc3a3093079b066e4c6471fa256b52788da0c927194147e\"\n",
    "\n",
    "# Spark config avec cluster (inspiré TP8)\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('KafkaToBronze') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-hadoop-cloud_2.12:3.5.3,io.delta:delta-spark_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.name\", \"filesystem\") \\\n",
    "    .set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .set(\"spark.default.parallelism\", \"10\") \\\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "# Config S3/Garage\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", GARAGE_ENDPOINT)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", GARAGE_ACCESS_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", GARAGE_SECRET_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint.region\", \"garage\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "print(\"Spark session créée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Lecture du stream Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture stream Kafka - Stories\n",
    "stories_stream = sql_context.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVERS) \\\n",
    "    .option(\"subscribe\", \"hn-stories\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Lecture stream Kafka - Comments\n",
    "comments_stream = sql_context.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVERS) \\\n",
    "    .option(\"subscribe\", \"hn-comments\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Streams Kafka connectés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 : Parsing des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, BooleanType\n",
    "\n",
    "# Schéma Stories\n",
    "story_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"by\", StringType()),\n",
    "    StructField(\"time\", LongType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"score\", IntegerType()),\n",
    "    StructField(\"descendants\", IntegerType()),\n",
    "    StructField(\"kids\", ArrayType(IntegerType()))\n",
    "])\n",
    "\n",
    "# Schéma Comments\n",
    "comment_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"by\", StringType()),\n",
    "    StructField(\"time\", LongType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"parent\", IntegerType()),\n",
    "    StructField(\"kids\", ArrayType(IntegerType())),\n",
    "    StructField(\"deleted\", BooleanType()),\n",
    "    StructField(\"dead\", BooleanType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser les stories\n",
    "stories_parsed = stories_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), story_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"time\"))) \\\n",
    "    .withColumn(\"_ingested_at\", current_timestamp())\n",
    "\n",
    "# Parser les comments\n",
    "comments_parsed = comments_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), comment_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"time\"))) \\\n",
    "    .withColumn(\"_ingested_at\", current_timestamp())\n",
    "\n",
    "print(\"Données parsées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : Écriture vers Bronze (avec déduplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "BRONZE_PATH = \"s3a://bronze/hackernews\"\n",
    "CHECKPOINT_PATH = \"s3a://bronze/checkpoints\"\n",
    "\n",
    "def upsert_to_delta(batch_df, batch_id, table_path):\n",
    "    \"\"\"Upsert (MERGE) vers Delta Lake - déduplique sur id\"\"\"\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Si la table existe, faire un MERGE\n",
    "    if DeltaTable.isDeltaTable(sql_context.sparkSession, table_path):\n",
    "        delta_table = DeltaTable.forPath(sql_context.sparkSession, table_path)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            batch_df.alias(\"source\"),\n",
    "            \"target.id = source.id\"\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    else:\n",
    "        # Première écriture : créer la table\n",
    "        batch_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Stories → Bronze (avec MERGE)\n",
    "stories_query = stories_parsed.writeStream \\\n",
    "    .foreachBatch(lambda df, id: upsert_to_delta(df, id, f\"{BRONZE_PATH}/stories\")) \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/stories\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Comments → Bronze (avec MERGE)\n",
    "comments_query = comments_parsed.writeStream \\\n",
    "    .foreachBatch(lambda df, id: upsert_to_delta(df, id, f\"{BRONZE_PATH}/comments\")) \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/comments\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Monitoring du stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Afficher les stats toutes les 10 secondes\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\n--- Stats ---\")\n",
    "        print(f\"Stories: {stories_query.status}\")\n",
    "        print(f\"Comments: {comments_query.status}\")\n",
    "        time.sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nArrêt des streams...\")\n",
    "    stories_query.stop()\n",
    "    comments_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les données Bronze\n",
    "sql_context.read.format(\"delta\").load(f\"{BRONZE_PATH}/stories\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_context.read.format(\"delta\").load(f\"{BRONZE_PATH}/comments\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
