{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer Spark Streaming → Bronze Layer\n",
    "\n",
    "Lit le flux Kafka en temps réel et écrit les données brutes dans Delta Lake (couche Bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 : Configuration Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Configuration\n",
    "# TODO : A remplacer par vos propres identifiants Garage\n",
    "KAFKA_SERVERS = \"kafka:9092\"\n",
    "GARAGE_ENDPOINT = \"http://garage:3900\"\n",
    "GARAGE_ACCESS_KEY = \"GK907b22f51dc0d0c5164474f2\"\n",
    "GARAGE_SECRET_KEY = \"6cf587853042d92d2cf6bb85b7c46a6a2400a47822e9baae32f9be0b7c5c9663\"\n",
    "\n",
    "# Spark config avec cluster (inspiré TP8)\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('KafkaToBronze') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-hadoop-cloud_2.12:3.5.3,io.delta:delta-spark_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.name\", \"filesystem\") \\\n",
    "    .set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .set(\"spark.default.parallelism\", \"10\") \\\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "# Config S3/Garage\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", GARAGE_ENDPOINT)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", GARAGE_ACCESS_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", GARAGE_SECRET_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint.region\", \"garage\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "print(\"Spark session créée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Lecture du stream Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture stream Kafka - Stories\n",
    "stories_stream = sql_context.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVERS) \\\n",
    "    .option(\"subscribe\", \"hn-stories\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Lecture stream Kafka - Comments\n",
    "comments_stream = sql_context.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVERS) \\\n",
    "    .option(\"subscribe\", \"hn-comments\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Streams Kafka connectés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 : Parsing des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, BooleanType\n",
    "\n",
    "# Schéma Stories\n",
    "story_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"by\", StringType()),\n",
    "    StructField(\"time\", LongType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"score\", IntegerType()),\n",
    "    StructField(\"descendants\", IntegerType()),\n",
    "    StructField(\"kids\", ArrayType(IntegerType()))\n",
    "])\n",
    "\n",
    "# Schéma Comments\n",
    "comment_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"by\", StringType()),\n",
    "    StructField(\"time\", LongType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"parent\", IntegerType()),\n",
    "    StructField(\"kids\", ArrayType(IntegerType())),\n",
    "    StructField(\"deleted\", BooleanType()),\n",
    "    StructField(\"dead\", BooleanType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser les stories\n",
    "stories_parsed = stories_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), story_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"time\"))) \\\n",
    "    .withColumn(\"_ingested_at\", current_timestamp())\n",
    "\n",
    "# Parser les comments\n",
    "comments_parsed = comments_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), comment_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"time\"))) \\\n",
    "    .withColumn(\"_ingested_at\", current_timestamp())\n",
    "\n",
    "print(\"Données parsées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 4 : Écriture vers Bronze (Append Only)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "BRONZE_PATH = \"s3a://bronze/hackernews\"\nCHECKPOINT_PATH = \"s3a://bronze/checkpoints\"\n\ndef append_to_bronze(batch_df, batch_id, table_path):\n    \"\"\"Append vers Bronze - pas de déduplication, écriture rapide\"\"\"\n    if batch_df.isEmpty():\n        return\n    batch_df.write.format(\"delta\").mode(\"append\").save(table_path)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stream Stories → Bronze (Append)\nstories_query = stories_parsed.writeStream \\\n    .foreachBatch(lambda df, id: append_to_bronze(df, id, f\"{BRONZE_PATH}/stories\")) \\\n    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/stories\") \\\n    .start()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stream Comments → Bronze (Append)\ncomments_query = comments_parsed.writeStream \\\n    .foreachBatch(lambda df, id: append_to_bronze(df, id, f\"{BRONZE_PATH}/comments\")) \\\n    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/comments\") \\\n    .start()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Monitoring du stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Afficher les stats toutes les 10 secondes\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"\\n--- Stats ---\")\n",
    "        print(f\"Stories: {stories_query.status}\")\n",
    "        print(f\"Comments: {comments_query.status}\")\n",
    "        time.sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nArrêt des streams...\")\n",
    "    stories_query.stop()\n",
    "    comments_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les données Bronze\n",
    "sql_context.read.format(\"delta\").load(f\"{BRONZE_PATH}/stories\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_context.read.format(\"delta\").load(f\"{BRONZE_PATH}/comments\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}