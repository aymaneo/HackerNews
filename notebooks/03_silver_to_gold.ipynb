{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver → Gold Layer (Spark NLP)\n",
    "\n",
    "Enrichissement des données Silver via Spark NLP : Sentiment Analysis, NER, Keyword Extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Spark + Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# TODO: A remplacer par vos propres identifiants Garage\n",
    "GARAGE_ENDPOINT = \"http://garage:3900\"\n",
    "GARAGE_ACCESS_KEY = \"GK2ae23cad2bbbf648143b1b8c\"\n",
    "GARAGE_SECRET_KEY = \"997e31832cbc9c78a2d919897f1cc9d63ad2c628464a7fba3a55f972c31790ee\"\n",
    "\n",
    "SILVER_PATH = \"s3a://silver/hackernews\"\n",
    "GOLD_PATH = \"s3a://gold/hackernews\"\n",
    "\n",
    "# TODO: Configurer volume partagé pour utiliser le cluster spark://spark:7077\n",
    "# Mode local pour Spark NLP (les modèles sont téléchargés localement)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SilverToGold-SparkNLP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n",
    "            \"io.delta:delta-spark_2.12:3.3.0,\"\n",
    "            \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", GARAGE_ENDPOINT)\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", GARAGE_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", GARAGE_SECRET_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.endpoint.region\", \"garage\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import (\n",
    "    Tokenizer, Normalizer, StopWordsCleaner, LemmatizerModel,\n",
    "    SentimentDLModel, NerDLModel, NerConverter,\n",
    "    SentenceDetector, WordEmbeddingsModel, UniversalSentenceEncoder\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, explode, explode_outer, length, desc, count, when, rank, round as round_, window\n",
    "from pyspark.sql.functions import sum as sum_\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lecture Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_silver = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/comments\")\n",
    "stories_silver = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/stories\")\n",
    "\n",
    "print(f\"Comments: {comments_silver.count()}, Stories: {stories_silver.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "Classifier chaque commentaire comme positif ou négatif. USE (Universal Sentence Encoder) produit directement des sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text_clean\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "use = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "sentiment_model = SentimentDLModel.pretrained(\"sentimentdl_use_imdb\", \"en\") \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "sentiment_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    use,\n",
    "    sentiment_model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_fitted = sentiment_pipeline.fit(comments_silver)\n",
    "comments_with_sentiment = sentiment_model_fitted.transform(comments_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment = comments_with_sentiment \\\n",
    "    .withColumn(\"sentiment_result\", explode(col(\"sentiment.result\"))) \\\n",
    "    .select(\"id\", \"by\", \"parent\", \"text_clean\", \"timestamp\", \"sentiment_result\")\n",
    "\n",
    "comments_sentiment.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NER (Named Entity Recognition)\n",
    "\n",
    "Extraire les entités nommées : personnes (PER), organisations (ORG), lieux (LOC). NER nécessite des word embeddings (GloVe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = NerDLModel.pretrained(\"ner_dl\", \"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "    .setOutputCol(\"entities\")\n",
    "\n",
    "ner_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    ner_model,\n",
    "    ner_converter\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_fitted = ner_pipeline.fit(comments_silver)\n",
    "comments_with_ner = ner_model_fitted.transform(comments_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_entities = comments_with_ner \\\n",
    "    .withColumn(\"entity\", explode_outer(col(\"entities\"))) \\\n",
    "    .select(\n",
    "        \"id\", \"by\", \"text_clean\",\n",
    "        col(\"entity.result\").alias(\"entity_text\"),\n",
    "        col(\"entity.metadata.entity\").alias(\"entity_type\")\n",
    "    )\n",
    "\n",
    "comments_entities.filter(col(\"entity_text\").isNotNull()).show(10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Keyword Extraction\n",
    "\n",
    "Identifier les mots-clés fréquents. Normalisation + suppression stop words + lemmatisation pour que \"Running\" et \"run\" soient comptés ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"cleanTokens\") \\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\", \"en\") \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCols([\"keywords\"]) \\\n",
    "    .setOutputAsArray(True)\n",
    "\n",
    "keywords_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    stopwords_cleaner,\n",
    "    lemmatizer,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_model_fitted = keywords_pipeline.fit(comments_silver)\n",
    "comments_with_keywords = keywords_model_fitted.transform(comments_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_exploded = comments_with_keywords \\\n",
    "    .select(\"id\", explode(col(\"keywords\")).alias(\"keyword\")) \\\n",
    "    .filter(length(col(\"keyword\")) > 2)\n",
    "\n",
    "top_keywords = keywords_exploded \\\n",
    "    .groupBy(\"keyword\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(50)\n",
    "\n",
    "top_keywords.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Requêtes en batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. SparkSQL - Sentiment par domaine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jointure comments + stories pour répondre à : *\"Quels sites génèrent les discussions les plus positives/négatives ?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment.createOrReplaceTempView(\"comments_sentiment\")\n",
    "stories_silver.createOrReplaceTempView(\"stories\")\n",
    "\n",
    "sentiment_by_domain = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.domain,\n",
    "        COUNT(*) as comment_count,\n",
    "        SUM(CASE WHEN c.sentiment_result = 'pos' THEN 1 ELSE 0 END) as positive,\n",
    "        SUM(CASE WHEN c.sentiment_result = 'neg' THEN 1 ELSE 0 END) as negative,\n",
    "        ROUND(SUM(CASE WHEN c.sentiment_result = 'pos' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as positive_pct\n",
    "    FROM comments_sentiment c\n",
    "    JOIN stories s ON c.parent = s.id\n",
    "    WHERE s.domain != ''\n",
    "    GROUP BY s.domain\n",
    "    HAVING COUNT(*) >= 5\n",
    "    ORDER BY comment_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "sentiment_by_domain.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Window Function - Classement des auteurs\n",
    "\n",
    "Utilisation de RANK() pour classer les auteurs par leur ratio de commentaires positifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégation par auteur : total comments, positifs, négatifs\n",
    "authors_stats = comments_sentiment \\\n",
    "    .groupBy(\"by\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_comments\"),\n",
    "        sum_(when(col(\"sentiment_result\") == \"pos\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "        sum_(when(col(\"sentiment_result\") == \"neg\", 1).otherwise(0)).alias(\"negative_count\")\n",
    "    ) \\\n",
    "    .filter(col(\"total_comments\") >= 3) \\\n",
    "    .withColumn(\"positive_ratio\", round_(col(\"positive_count\") * 100.0 / col(\"total_comments\"), 2))\n",
    "\n",
    "# Window function : RANK par ratio de positivité\n",
    "window_spec = Window.orderBy(desc(\"positive_ratio\"), desc(\"total_comments\"))\n",
    "\n",
    "authors_ranked = authors_stats \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 20)\n",
    "\n",
    "authors_ranked.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Requêtes en streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_stream = spark.readStream.format(\"delta\").load(f\"{SILVER_PATH}/comments\")\n",
    "comments_with_sentiment = sentiment_model_fitted.transform(comments_stream)                                                                                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Sentiment en temps réel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrégation du sentiment des commentaires par fenêtre de 5minutes:                                                                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_windowed = comments_with_sentiment \\\n",
    "    .withColumn(\"sentiment_result\", explode(col(\"sentiment.result\"))) \\\n",
    "    .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\n",
    "            col(\"timestamp\"),\n",
    "            \"5 minutes\",\n",
    "        )\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_comments\"),\n",
    "        sum_(when(col(\"sentiment_result\") == \"pos\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "        sum_(when(col(\"sentiment_result\") == \"neg\", 1).otherwise(0)).alias(\"negative_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"positive_ratio\", round_(col(\"positive_count\") * 100.0 / col(\"total_comments\"), 2))\n",
    "\n",
    "sentiment_windowed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{GOLD_PATH}/_checkpoints/sentiment_windowed\") \\\n",
    "    .start(f\"{GOLD_PATH}/sentiment_real_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top mots-clés avec fenêtre glissante de 10 min, mise à jour toutes les 2 min:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_with_kw = keywords_model_fitted.transform(comments_stream)\n",
    "keywords_exploded_stream = comments_with_kw \\\n",
    "    .select(\n",
    "        col(\"id\"),\n",
    "        col(\"timestamp\"),\n",
    "        explode(col(\"keywords\")).alias(\"keyword\")\n",
    "    ) \\\n",
    "    .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "    .groupBy(\n",
    "        window(\n",
    "            col(\"timestamp\"),\n",
    "            \"10 minutes\",\n",
    "            \"2 minutes\"\n",
    "        ),\n",
    "        col(\"keyword\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "keywords_exploded_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", f\"{GOLD_PATH}/_checkpoints/keywords_windowed\") \\\n",
    "    .start(f\"{GOLD_PATH}/keywords_real_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisation Pandas + Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_df = comments_sentiment.groupBy(\"sentiment_result\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=sentiment_df, x=\"sentiment_result\", y=\"count\", palette=\"viridis\")\n",
    "plt.title(\"Distribution des sentiments dans les commentaires HackerNews\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Nombre de commentaires\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_entities_df = comments_entities \\\n",
    "    .filter(col(\"entity_type\").isin([\"ORG\", \"PRODUCT\", \"PERSON\"])) \\\n",
    "    .groupBy(\"entity_text\", \"entity_type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(15) \\\n",
    "    .toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top_entities_df, x=\"count\", y=\"entity_text\", hue=\"entity_type\", dodge=False)\n",
    "plt.title(\"Top entités mentionnées dans les commentaires HackerNews\")\n",
    "plt.xlabel(\"Nombre de mentions\")\n",
    "plt.ylabel(\"Entité\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords_df = top_keywords.limit(20).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top_keywords_df, x=\"count\", y=\"keyword\", palette=\"magma\")\n",
    "plt.title(\"Top 20 mots-clés dans les commentaires HackerNews\")\n",
    "plt.xlabel(\"Fréquence\")\n",
    "plt.ylabel(\"Mot-clé\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_keywords.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Écriture Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/comments_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_aggregated = comments_entities \\\n",
    "    .filter(col(\"entity_text\").isNotNull()) \\\n",
    "    .groupBy(\"entity_text\", \"entity_type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "entities_aggregated.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_domain.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/sentiment_by_domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/comments_sentiment\").show(5, truncate=40)\n",
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/entities\").show(10)\n",
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/keywords\").show(10)\n",
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/sentiment_by_domain\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
