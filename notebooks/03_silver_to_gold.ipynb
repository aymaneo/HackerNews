{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver → Gold Layer (Spark NLP)\n",
    "\n",
    "Enrichissement des données Silver via Spark NLP : Sentiment Analysis, NER, Keyword Extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Spark + Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "GARAGE_ENDPOINT = \"http://garage:3900\"\n",
    "GARAGE_ACCESS_KEY = \"GKa25124b4fd82613c063217f3\"\n",
    "GARAGE_SECRET_KEY = \"008126399688f9b1efc3a3093079b066e4c6471fa256b52788da0c927194147e\"\n",
    "\n",
    "SILVER_PATH = \"s3a://silver/hackernews\"\n",
    "GOLD_PATH = \"s3a://gold/hackernews\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SilverToGold-SparkNLP\") \\\n",
    "    .master(\"spark://spark:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n",
    "            \"io.delta:delta-spark_2.12:3.3.0,\"\n",
    "            \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", GARAGE_ENDPOINT)\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", GARAGE_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", GARAGE_SECRET_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.endpoint.region\", \"garage\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import (\n",
    "    Tokenizer, Normalizer, StopWordsCleaner, LemmatizerModel,\n",
    "    SentimentDLModel, NerDLModel, NerConverter,\n",
    "    SentenceDetector, WordEmbeddingsModel\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, explode, explode_outer, size, desc\n",
    "\n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lecture Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_silver = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/comments\")\n",
    "stories_silver = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/stories\")\n",
    "\n",
    "print(f\"Comments: {comments_silver.count()}, Stories: {stories_silver.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "Classifier chaque commentaire comme positif ou négatif. GloVe embeddings permettent au modèle de comprendre le sens sémantique des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text_clean\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "sentiment_model = SentimentDLModel.pretrained(\"sentimentdl_glove_imdb\", \"en\") \\\n",
    "    .setInputCols([\"sentence\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "sentiment_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    sentiment_model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_fitted = sentiment_pipeline.fit(comments_silver)\n",
    "comments_with_sentiment = sentiment_model_fitted.transform(comments_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment = comments_with_sentiment \\\n",
    "    .withColumn(\"sentiment_result\", explode(col(\"sentiment.result\"))) \\\n",
    "    .select(\"id\", \"by\", \"parent\", \"text_clean\", \"timestamp\", \"sentiment_result\")\n",
    "\n",
    "comments_sentiment.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NER (Named Entity Recognition)\n",
    "\n",
    "Extraire les entités nommées : personnes (PER), organisations (ORG), lieux (LOC). Permet d'identifier les entreprises et personnalités tech les plus discutées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = NerDLModel.pretrained(\"ner_dl\", \"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "    .setOutputCol(\"entities\")\n",
    "\n",
    "ner_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    ner_embeddings,\n",
    "    ner_model,\n",
    "    ner_converter\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_fitted = ner_pipeline.fit(comments_silver)\n",
    "comments_with_ner = ner_model_fitted.transform(comments_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_entities = comments_with_ner \\\n",
    "    .withColumn(\"entity\", explode_outer(col(\"entities\"))) \\\n",
    "    .select(\n",
    "        \"id\", \"by\", \"text_clean\",\n",
    "        col(\"entity.result\").alias(\"entity_text\"),\n",
    "        col(\"entity.metadata.entity\").alias(\"entity_type\")\n",
    "    )\n",
    "\n",
    "comments_entities.filter(col(\"entity_text\").isNotNull()).show(10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Keyword Extraction\n",
    "\n",
    "Identifier les mots-clés fréquents. Normalisation + suppression stop words + lemmatisation pour que \"Running\" et \"run\" soient comptés ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"cleanTokens\") \\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\", \"en\") \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCols([\"keywords\"]) \\\n",
    "    .setOutputAsArray(True)\n",
    "\n",
    "keywords_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    stopwords_cleaner,\n",
    "    lemmatizer,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_model_fitted = keywords_pipeline.fit(comments_silver)\n",
    "comments_with_keywords = keywords_model_fitted.transform(comments_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_exploded = comments_with_keywords \\\n",
    "    .select(\"id\", explode(col(\"keywords\")).alias(\"keyword\")) \\\n",
    "    .filter(size(col(\"keyword\")) > 2)\n",
    "\n",
    "top_keywords = keywords_exploded \\\n",
    "    .groupBy(\"keyword\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(50)\n",
    "\n",
    "top_keywords.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SparkSQL - Sentiment par domaine\n",
    "\n",
    "Jointure comments + stories pour répondre à : *\"Quels sites génèrent les discussions les plus positives/négatives ?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment.createOrReplaceTempView(\"comments_sentiment\")\n",
    "stories_silver.createOrReplaceTempView(\"stories\")\n",
    "\n",
    "sentiment_by_domain = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.domain,\n",
    "        COUNT(*) as comment_count,\n",
    "        SUM(CASE WHEN c.sentiment_result = 'pos' THEN 1 ELSE 0 END) as positive,\n",
    "        SUM(CASE WHEN c.sentiment_result = 'neg' THEN 1 ELSE 0 END) as negative,\n",
    "        ROUND(SUM(CASE WHEN c.sentiment_result = 'pos' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as positive_pct\n",
    "    FROM comments_sentiment c\n",
    "    JOIN stories s ON c.parent = s.id\n",
    "    WHERE s.domain != ''\n",
    "    GROUP BY s.domain\n",
    "    HAVING COUNT(*) >= 5\n",
    "    ORDER BY comment_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "sentiment_by_domain.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation Pandas + Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_df = comments_sentiment.groupBy(\"sentiment_result\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=sentiment_df, x=\"sentiment_result\", y=\"count\", palette=\"viridis\")\n",
    "plt.title(\"Distribution des sentiments dans les commentaires HackerNews\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Nombre de commentaires\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sentiment_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_entities_df = comments_entities \\\n",
    "    .filter(col(\"entity_type\").isin([\"ORG\", \"PRODUCT\", \"PERSON\"])) \\\n",
    "    .groupBy(\"entity_text\", \"entity_type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(15) \\\n",
    "    .toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top_entities_df, x=\"count\", y=\"entity_text\", hue=\"entity_type\", dodge=False)\n",
    "plt.title(\"Top entités mentionnées dans les commentaires HackerNews\")\n",
    "plt.xlabel(\"Nombre de mentions\")\n",
    "plt.ylabel(\"Entité\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_entities.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords_df = top_keywords.limit(20).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top_keywords_df, x=\"count\", y=\"keyword\", palette=\"magma\")\n",
    "plt.title(\"Top 20 mots-clés dans les commentaires HackerNews\")\n",
    "plt.xlabel(\"Fréquence\")\n",
    "plt.ylabel(\"Mot-clé\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_keywords.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Écriture Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/comments_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_aggregated = comments_entities \\\n",
    "    .filter(col(\"entity_text\").isNotNull()) \\\n",
    "    .groupBy(\"entity_text\", \"entity_type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "entities_aggregated.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_domain.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/sentiment_by_domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/comments_sentiment\").show(5, truncate=40)\n",
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/entities\").show(10)\n",
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/keywords\").show(10)\n",
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/sentiment_by_domain\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
