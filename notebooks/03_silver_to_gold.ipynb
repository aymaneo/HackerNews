{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver → Gold Layer (Spark NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Spark + Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "GARAGE_ENDPOINT = \"http://garage:3900\"\n",
    "GARAGE_ACCESS_KEY = \"GKa25124b4fd82613c063217f3\"\n",
    "GARAGE_SECRET_KEY = \"008126399688f9b1efc3a3093079b066e4c6471fa256b52788da0c927194147e\"\n",
    "\n",
    "SILVER_PATH = \"s3a://silver/hackernews\"\n",
    "GOLD_PATH = \"s3a://gold/hackernews\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SilverToGold-SparkNLP\") \\\n",
    "    .master(\"spark://spark:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n",
    "            \"io.delta:delta-spark_2.12:3.3.0,\"\n",
    "            \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", GARAGE_ENDPOINT)\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", GARAGE_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", GARAGE_SECRET_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.endpoint.region\", \"garage\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sparknlp\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import (\n    Tokenizer, SentimentDLModel, NerDLModel, NerConverter,\n    SentenceDetector, WordEmbeddingsModel\n)\nfrom pyspark.ml import Pipeline\n\nprint(f\"Spark NLP version: {sparknlp.version()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lecture Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_silver = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/comments\")\n",
    "stories_silver = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/stories\")\n",
    "\n",
    "print(f\"Comments: {comments_silver.count()}, Stories: {stories_silver.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text_clean\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "sentiment_model = SentimentDLModel.pretrained(\"sentimentdl_glove_imdb\", \"en\") \\\n",
    "    .setInputCols([\"sentence\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "sentiment_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    sentiment_model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.functions import col, explode\n\ncomments_sentiment = comments_with_sentiment \\\n    .withColumn(\"sentiment_result\", explode(col(\"sentiment.result\"))) \\\n    .select(\"id\", \"by\", \"parent\", \"text_clean\", \"timestamp\", \"sentiment_result\")\n\ncomments_sentiment.show(5, truncate=50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, expr\n",
    "\n",
    "comments_sentiment = comments_with_sentiment \\\n",
    "    .withColumn(\"sentiment_result\", explode(col(\"sentiment.result\"))) \\\n",
    "    .select(\"id\", \"by\", \"parent\", \"text_clean\", \"timestamp\", \"sentiment_result\")\n",
    "\n",
    "comments_sentiment.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.functions import explode_outer\n\ncomments_entities = comments_with_ner \\\n    .withColumn(\"entity\", explode_outer(col(\"entities\"))) \\\n    .select(\n        \"id\", \"by\", \"text_clean\",\n        col(\"entity.result\").alias(\"entity_text\"),\n        col(\"entity.metadata.entity\").alias(\"entity_type\")\n    )\n\ncomments_entities.filter(col(\"entity_text\").isNotNull()).show(10, truncate=40)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_fitted = ner_pipeline.fit(comments_silver)\n",
    "comments_with_ner = ner_model_fitted.transform(comments_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode_outer, size\n",
    "\n",
    "comments_entities = comments_with_ner \\\n",
    "    .withColumn(\"entity\", explode_outer(col(\"entities\"))) \\\n",
    "    .select(\n",
    "        \"id\", \"by\", \"text_clean\",\n",
    "        col(\"entity.result\").alias(\"entity_text\"),\n",
    "        col(\"entity.metadata.entity\").alias(\"entity_type\")\n",
    "    )\n",
    "\n",
    "comments_entities.filter(col(\"entity_text\").isNotNull()).show(10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Requête SparkSQL - Sentiment par domaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment.createOrReplaceTempView(\"comments_sentiment\")\n",
    "stories_silver.createOrReplaceTempView(\"stories\")\n",
    "\n",
    "sentiment_by_domain = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.domain,\n",
    "        COUNT(*) as comment_count,\n",
    "        SUM(CASE WHEN c.sentiment_result = 'pos' THEN 1 ELSE 0 END) as positive,\n",
    "        SUM(CASE WHEN c.sentiment_result = 'neg' THEN 1 ELSE 0 END) as negative,\n",
    "        ROUND(SUM(CASE WHEN c.sentiment_result = 'pos' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as positive_pct\n",
    "    FROM comments_sentiment c\n",
    "    JOIN stories s ON c.parent = s.id\n",
    "    WHERE s.domain != ''\n",
    "    GROUP BY s.domain\n",
    "    HAVING COUNT(*) >= 5\n",
    "    ORDER BY comment_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "sentiment_by_domain.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation Pandas + Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_df = comments_sentiment.groupBy(\"sentiment_result\").count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=sentiment_df, x=\"sentiment_result\", y=\"count\", palette=\"viridis\")\n",
    "plt.title(\"Distribution des sentiments dans les commentaires HackerNews\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Nombre de commentaires\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sentiment_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_entities = comments_entities \\\n",
    "    .filter(col(\"entity_type\").isin([\"ORG\", \"PRODUCT\", \"PERSON\"])) \\\n",
    "    .groupBy(\"entity_text\", \"entity_type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(15) \\\n",
    "    .toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top_entities, x=\"count\", y=\"entity_text\", hue=\"entity_type\", dodge=False)\n",
    "plt.title(\"Top entités mentionnées dans les commentaires HackerNews\")\n",
    "plt.xlabel(\"Nombre de mentions\")\n",
    "plt.ylabel(\"Entité\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_entities.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Écriture Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/comments_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_aggregated = comments_entities \\\n",
    "    .filter(col(\"entity_text\").isNotNull()) \\\n",
    "    .groupBy(\"entity_text\", \"entity_type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "entities_aggregated.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_domain.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_PATH}/sentiment_by_domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/comments_sentiment\").show(5, truncate=40)\n",
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/entities\").show(10)\n",
    "spark.read.format(\"delta\").load(f\"{GOLD_PATH}/sentiment_by_domain\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}